{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "axSFmwVUoGtP"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}"
      ],
      "metadata": {
        "id": "6jqkA06MoZJ9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ASSIGNMENT 1"
      ],
      "metadata": {
        "id": "UmedYT3Brwmj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model)\n",
        "        output = generator(prompt, max_length=30)\n",
        "        print(output[0][\"generated_text\"])\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FElatY-ofcZ",
        "outputId": "302abeac-2dde-42d1-dda7-4590785e3b1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n",
            "\n",
            "RoBERTa:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is\n",
            "\n",
            "BART:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence isCornIESIES suitWHATIESIESIESaim Ether Etherwhen MajBitcoineligibleCornemouthWHATIES DublinDragon sceneryIESIES UnleashedoilCornCornnesses Voltageeligibleimil Conferencepperc archaeologists deliberately� decreases interpretriblepperc restroomsWHATDragonDragonroiteligible�inez� Highly Brawlrible Examplesinez sever sever portrays punchesrible Mysteries commissionersinezistaIESisheseligibleriblereleased punchesrible refrigeratorWHATWHAT ExamplesWHATrible refrigerator academyWHAT commissioners Billboardyt deliberatelyCorniar portrays workshops refrigeratoreligible Billboard� refrigeratoreligible portraysinezriblerites JFKrible deliberately Hazardrible portrays deliberatelyCorn Hazard portrays McInt heads Moran portraysiresribleWHAT deliberatelyeligible deliberatelyuddenyteligibleeligibleWHAT portrays�rible streets interpretrible Mysteriesombiesriticeligible Hazard portrays brothers humanity Mysteries Sonyreleased deliberately Sonyiareligible HazardWHAT indicated Sony Hazard Hazard streets Sony institribleinezlivionnea streets Sony Mysteries Sonynea Moran portrays streets interpret Mysteries Mysteries Sonyombiesinezrible interpret humanity streets interpret Billboard Sony Mysterieseland Sony Sonyires proseombiesimilrible McInteland streets JFKnea portrays McInt McInt streets streets heeligible--+ Sonyimil subscriptioneland headseland deliberately McInt Fuckriticeland Mysteriesiarombies Fuck streets deliberately Mysteries95 JFKnea streets JFK streets streets95 Memphis Eternal streets Mysteries streets gradual Mysterieseland JFK JFK portrays successfully JFK streetsombies streets interpret persist persisteland streets persist persist streets persistneaeligibleeligible portrays\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ASSIGNMENT2"
      ],
      "metadata": {
        "id": "PiDmgnBhpK3M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = {\n",
        "    \"BERT\": \"The goal of Generative AI is to [MASK] new content.\",\n",
        "    \"RoBERTa\": \"The goal of Generative AI is to <mask> new content.\",\n",
        "    \"BART\": \"The goal of Generative AI is to <mask> new content.\"\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    try:\n",
        "        fill = pipeline(\"fill-mask\", model=model)\n",
        "        outputs = fill(sentences[name])\n",
        "        for o in outputs[:3]:\n",
        "            print(o[\"token_str\"], \"-\", round(o[\"score\"], 4))\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPzasAcEpW7l",
        "outputId": "fecfce3f-b2bf-4803-cc13-7417ac19acfe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create - 0.5397\n",
            "generate - 0.1558\n",
            "produce - 0.0541\n",
            "\n",
            "RoBERTa:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " generate - 0.3711\n",
            " create - 0.3677\n",
            " discover - 0.0835\n",
            "\n",
            "BART:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " create - 0.0746\n",
            " help - 0.0657\n",
            " provide - 0.0609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ASSIGNMENT 3"
      ],
      "metadata": {
        "id": "viQT587ZrOHp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=model)\n",
        "        answer = qa(question=question, context=context)\n",
        "        print(answer)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3EEXnHWrGAv",
        "outputId": "edb50437-2dcc-47f9-d0c8-4086353de164"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.008368083508685231, 'start': 14, 'end': 60, 'answer': 'poses significant risks such as hallucinations'}\n",
            "\n",
            "RoBERTa:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.007620341144502163, 'start': 72, 'end': 82, 'answer': 'deepfakes.'}\n",
            "\n",
            "BART:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.02815950382500887, 'start': 68, 'end': 81, 'answer': 'and deepfakes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WzjCtMZEr4_5"
      }
    }
  ]
}